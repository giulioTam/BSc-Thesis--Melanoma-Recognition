{"cells":[{"cell_type":"markdown","metadata":{"id":"H9ci3Xpz4Q7z"},"source":["# **IMPORTAZIONE LIBRERIE**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZGF-9ythWSJ"},"outputs":[],"source":["import csv\n","import shutil\n","import os\n","import cv2\n","from PIL import Image\n","import numpy as np\n","import random\n","import time\n","import pandas as pd\n","from glob import glob\n","\n","!pip install dataframe_image\n","import dataframe_image as dfi\n","\n","from keras.utils import normalize, image_dataset_from_directory\n","import matplotlib.pyplot as plt\n","\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Model, Sequential\n","from keras.layers import Input, Dense, Flatten, add\n","from keras.layers import RandomFlip, RandomRotation, RandomZoom, \\\n","                         RandomTranslation, Rescaling, Resizing\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from keras.applications.vgg16 import VGG16\n","from keras.optimizers import Adam, SGD\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils import img_to_array, load_img\n","from keras.metrics import Recall, Precision, AUC\n","from keras.callbacks import EarlyStopping\n","from keras.wrappers.scikit_learn import KerasClassifier\n","\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import GridSearchCV, StratifiedKFold\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,roc_curve,\\\n","  auc, accuracy_score, average_precision_score, balanced_accuracy_score, \\\n","  precision_recall_curve, f1_score, make_scorer\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsdRseByzD9A"},"outputs":[],"source":["print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"]},{"cell_type":"markdown","source":["#**PREPARAZIONE DEI DATI**"],"metadata":{"id":"ICf8sW6K_YtR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzOlWRk0w1Ap"},"outputs":[],"source":["ISIC_path = 'drive/MyDrive/Colab Notebooks/ISIC_challenge/'\n","train_dir = ISIC_path + 'datasets/training/'\n","test_dir = ISIC_path + 'datasets/test/'\n","csv_dir = ISIC_path + 'csv/'\n","results_dir = ISIC_path + 'results/'"]},{"cell_type":"code","source":["\"\"\"\n","# DIVIDERE LE IMMAGINI DEL TRAINING E TEST SET IN DUE CLASSI (CARTELLE)\n","# Leggendo il file csv, sposto le immagini nelle rispettive cartelle (benigno o maligno)\n","def move_ben_mal(file_csv, dest):\n","  with open(file_csv) as file_obj:\n","  reader = csv.reader(file_obj)\n","\n","  # itero ogni riga del file csv\n","  for row in reader:\n","    if  os.path.exists(dest+row[0]+'.jpg'):\n","      if row[1] == 'benign' or row[1] == \"0.0\":\n","        shutil.move(dest+row[0]+'.jpg', dest+'Benigno')\n","      elif row[1] == 'malignant' or row[1] == \"1.0\":\n","        shutil.move(dest+row[0]+'.jpg', dest+'Maligno')\n","      else:\n","        print(\"errore: label non riconosciuta\")\n","\n","\n","move_ben_mal(csv_dir + 'ISBI2016_ISIC_Part3_Training_GroundTruth.csv', train_dir)\n","move_ben_mal(csv_dir+'ISBI2016_ISIC_Part3_Test_GroundTruth.csv', test_dir)\n","\"\"\""],"metadata":{"id":"yWArkLQVfMRw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzione per plottare un istogramma dato il numero di immagini per ogni classe\n","def plot_histogram(n_class_1, n_class_2, title=\"\"):\n","    fig, ax = plt.subplots()\n","    ax.bar([0, 1], [n_class_1, n_class_2], color=['royalblue', 'darkorange'])\n","\n","    ax.set_xticks([0, 1])\n","    ax.set_xticklabels(['benign', 'malignant'])\n","    ax.set_xlabel('Classi')\n","    ax.set_ylabel('Numero di immagini')\n","    plt.title(title+ ' Histogram')\n","    plt.show()"],"metadata":{"id":"zqwjcUqFyQ_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzvjsNjmI5C_"},"outputs":[],"source":["# controllo la grandezza di ogni cartella\n","# ------ Training set---------\n","b = len(os.listdir(train_dir+'Benigno'))\n","m = len(os.listdir(train_dir+'Maligno'))\n","LEN_TRAIN = b + m\n","print(\"Numero di immagini totali contenute nel training set = \", LEN_TRAIN)\n","print(f\"# immagini di nei benigni = {b}\")\n","print(f\"# immagini di nei maligni = {m}\\n\")\n","\n","plot_histogram(b, m, 'Training set')\n","\n","\n","# ------ Test set---------\n","b = len(os.listdir(test_dir+'Benigno'))\n","m = len(os.listdir(test_dir+'Maligno'))\n","LEN_TEST = b + m\n","print(\"\\n\\n\\nNumero di immagini totali contenute nel test set = \", LEN_TEST)\n","print(f\"# immagini di nei benigni = {b}\")\n","print(f\"# immagini di nei maligni = {m}\\n\")\n","\n","plot_histogram(b, m, 'Test set')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxF7YCxQoEPa"},"outputs":[],"source":["#stampo un esempio  di un neo maligno preso casualmente dal trainingset\n","l = os.listdir(train_dir+'Maligno')\n","m=random.randint(0, len(l))\n","print(\"Un esempio di neo maligno è:\")\n","plt.figure(figsize=(5,7))\n","plt.axis('off')\n","plt.imshow(load_img(train_dir + 'Maligno/' + l[m]))\n","plt.show()\n","\n","#stampo un esempio  di un neo benigno preso casualmente dal trainingset\n","l = os.listdir(train_dir+'Benigno')\n","m=random.randint(0, len(l))\n","print(\"\\n\\n\\nUn esempio di neo benigno è:\")\n","plt.figure(figsize=(5,7))\n","plt.axis('off')\n","plt.imshow(load_img(train_dir + 'Benigno/' + l[m]))\n","plt.show()"]},{"cell_type":"markdown","source":["# **NO AUGMENTATION**"],"metadata":{"id":"kxAi4NmJV3HL"}},{"cell_type":"code","source":["IMAGE_SIZE = [224,224]\n","\n","#-----------train_dataset_generator-------------\n","train_generator = image_dataset_from_directory(\n","    train_dir,\n","    color_mode='rgb',\n","    image_size=IMAGE_SIZE,\n","    batch_size=LEN_TRAIN,\n","    label_mode='binary',\n",")\n","\n","\n","#----------test_dataset_generator------------\n","test_generator = image_dataset_from_directory(\n","    test_dir,\n","    image_size=IMAGE_SIZE,\n","    batch_size=LEN_TEST,\n","    label_mode='binary'\n",")\n","\n","print(\"\\nLe classi sono: \", test_generator.class_names)"],"metadata":{"id":"jzgb84Hhp6pR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# funzione che prende in input un dataset e ritorna due array, uno con\n","# le immagini sottoforma di array di float, uno con le label associate\n","################################################################################\n","def from_data_to_array(data_generator):\n","  data = []\n","  target = []\n","  for x,y in data_generator:\n","    data.append(x)\n","    target.append(y)\n","\n","  data = np.squeeze(data)\n","  # crea un numpy array che contiene tutte le immagini\n","  data = np.array(data)\n","  # Normalizza i valori dei pixel nell'intervallo [0, 1]\n","  data = data.astype('float32') / 255.0\n","\n","  target = np.squeeze(target)\n","  # crea un numpy array che contiene tutte le label\n","  target = np.array(target)\n","  return data, target\n","\n","\n","train_data,train_target = from_data_to_array(train_generator)\n","print('train_data shape = ',np.shape(train_data))\n","print('train_target shape = ',train_target.shape)\n","test_data,test_target = from_data_to_array(test_generator)\n","print('test_data shape = ',np.shape(test_data))\n","print('test_target shape = ',test_target.shape)"],"metadata":{"id":"w_IkYlepeYn6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **DATA AUGMENTATION**"],"metadata":{"id":"awyelS56zzRV"}},{"cell_type":"code","source":["IMAGE_SIZE = [224,224]\n","\n","\n","# ImageDataGenerator per il data augmentation\n","datagen = ImageDataGenerator(rotation_range=90,\n","                             shear_range=15,\n","                             zoom_range=0.2,\n","                             fill_mode='reflect',\n","                             vertical_flip=True,\n","                             horizontal_flip=True,\n","                             )\n","\n","\n","train_data = []\n","train_target = []\n","\n","for sub_dir in os.listdir(train_dir):\n","  if sub_dir == 'Maligno':\n","    label = 1.\n","    # numero di immagini generate per ogni immagine originale\n","    num_augmented_images = 4\n","  else:\n","    label = 0.\n","    num_augmented_images = 1\n","\n","  for filename in os.listdir(os.path.join(train_dir, sub_dir)):\n","    img = load_img(os.path.join(train_dir, sub_dir, filename), target_size=IMAGE_SIZE)\n","    img_array = img_to_array(img)\n","    img_array = np.expand_dims(img_array, axis=0)\n","    # Aggiungo l'immagine originale\n","    train_data.append(img_array[0])\n","    train_target.append(label)\n","\n","    # Aggiungo le immagini modificate\n","    i = 0\n","    for x_batch, y_batch in datagen.flow(img_array, [0], batch_size=1):\n","      if i >= num_augmented_images:\n","        break\n","      train_data.append(x_batch[0])\n","      train_target.append(label)\n","      i += 1\n","\n","\n","# crea un numpy array che contiene tutte le immagini\n","train_data = np.array(train_data)\n","\n","# normalizza i valori dei pixel tra 0 e 1\n","train_data = train_data.astype('float32') / 255.0\n","\n","# crea un numpy array che contiene tutte le label\n","train_target = np.array(train_target)\n","\n","b =  np.sum(train_target == 0)\n","m = np.sum(train_target == 1)\n","print('Numero totale (dopo l\\'augmentation) di immagini per il training set:', train_data.shape[0])\n","print('Numero di immagini per la classe \\'Benigno\\':', b)\n","print('Numero di immagini per la classe \\'Maligno\\':', m)\n","print('\\n\\n\\n')\n","plot_histogram(b, m, 'Augmented Training set')"],"metadata":{"id":"Tr5I-UcmALoA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------shuffle del train_data-----------------------------------------\n","indices = np.arange(len(train_data))\n","\n","# mescolamento degli indici in modo randomico\n","np.random.shuffle(indices)\n","\n","train_data = train_data[indices]\n","train_target = train_target[indices]"],"metadata":{"id":"y3a1hOvsA5_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faccio la stessa cosa per il test set, ma senza effettuare augmentation\n","\n","test_data = []\n","test_target = []\n","\n","for sub_dir in os.listdir(test_dir):\n","  label = 1 if sub_dir == 'Maligno' else 0\n","\n","  for filename in os.listdir(os.path.join(test_dir, sub_dir)):\n","    img = load_img(os.path.join(test_dir, sub_dir, filename), target_size=IMAGE_SIZE)\n","    x = img_to_array(img)\n","    test_data.append(x)\n","    test_target.append(label)\n","\n","\n","# Crea un array numpy che contiene tutte le immagini\n","test_data = np.array(test_data)\n","\n","# Normalizza i valori dei pixel nell'intervallo [0, 1]\n","test_data = test_data.astype('float32') / 255.0\n","\n","# Crea un array numpy che contiene tutte le label\n","test_target = np.array(test_target)\n","\n","# Stampa il numero totale di immagini e il numero di immagini per ogni classe\n","print('Numero totale di immagini per il test set:', test_data.shape[0])\n","print('Numero di immagini per la classe \\'Benigno\\':', np.sum(test_target == 0))\n","print('Numero di immagini per la classe \\'Maligno\\':', np.sum(test_target == 1))"],"metadata":{"id":"LIZa3Atf2NHw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-AeRRlk39xW"},"source":["# **HYPERPARAMETER OPTIMIZATION**"]},{"cell_type":"code","source":["class_weights = compute_class_weight('balanced', classes=np.unique(train_target), y=train_target)\n","class_weights = {0: class_weights[0], 1: class_weights[1]}\n","print(class_weights)\n"],"metadata":{"id":"KPhwzxdmgBdz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# definire il modello di base\n","def create_model():\n","  model = keras.Sequential()\n","  model.add(ResNet50(input_shape=IMAGE_SIZE+[3], include_top=False))\n","  model.add(Flatten())\n","  model.add(Dense(1, activation='sigmoid'))\n","  model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=Adam(learning_rate=0.0001),\n","    metrics=['accuracy']\n","  )\n","  return model\n","\n","\n","model = KerasClassifier(build_fn=create_model)\n","\n","# parametri della griglia di ricerca\n","epochs = [25, 30, 35, 40]\n","batch_size = [16, 32, 64]\n","param_grid = {'epochs': epochs, 'batch_size': batch_size}\n","\n","\n","scorer = make_scorer(balanced_accuracy_score)\n","\n","grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, verbose=1)\n","grid_result = grid.fit(train_data, train_target, class_weight=class_weights, shuffle=True)\n","\n","\n","# stampa dei risultati\n","print(grid_result)\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"],"metadata":{"id":"bCeDRlf14hZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# definire il modello di base\n","def create_model(optimizer, lr):\n","  model = keras.Sequential()\n","  model.add(ResNet50(input_shape=IMAGE_SIZE+[3], include_top=False))\n","  model.add(Flatten())\n","  model.add(Dense(1, activation='sigmoid'))\n","  if optimizer == 'adam':\n","      opt = Adam(learning_rate=lr)\n","  elif optimizer == 'sgd':\n","      opt = SGD(learning_rate=lr)\n","  else:\n","      raise ValueError('Optimizer non valido:', optimizer)\n","\n","  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  return model\n","\n","\n","\n","def hp_tuning(train_data, train_target):\n","  # dizionario con tutti i parametri da ottimizzare\n","  params = {'optimizer': ['adam', 'sgd'], 'learning_rate': [0.0001, 0.001, 0.01]}\n","\n","  kc = KerasClassifier(build_fn=create_model, epochs=35)\n","\n","  scorer = make_scorer(balanced_accuracy_score)\n","\n","  grid = GridSearchCV(kc, param_grid=params, scoring=scorer, verbose=1)\n","  grid_res = grid.fit(train_data, train_target, class_weight=class_weights, shuffle=True)\n","\n","  # Risultati\n","  print(\"Best: %f using %s\" % (grid_res.best_score_, grid_res.best_params_))\n","  return grid_res.best_params_"],"metadata":{"id":"q7sVJnjUO1Eq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hp_tuning(train_data, train_target)"],"metadata":{"id":"Pq7NK9y2R-kq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **CREAZIONE MODELLO: VGG16**"],"metadata":{"id":"qzy-OFqxFz5M"}},{"cell_type":"code","source":["LEARNING_RATE = 1e-4\n","pretrained_mod = 'VGG16/'\n","\n","# creo il modello\n","vgg = VGG16(input_shape=IMAGE_SIZE+[3], include_top=False)\n","x = Flatten()(vgg.output)\n","prediction = Dense(1, activation='sigmoid')(x)  #nell'output layer avrò un solo nodo (output binario)\n","model = Model(inputs=vgg.input, outputs=prediction)\n","\n","# compilo il modello\n","model.compile(\n","  loss='binary_crossentropy',\n","  optimizer=Adam(learning_rate=LEARNING_RATE),\n","  metrics=['accuracy', AUC(name='auc')]\n",")"],"metadata":{"id":"Wa3Qyfa1F9Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.utils.vis_utils import plot_model\n","plot_model(model,show_shapes=True, to_file=results_dir+'VGG16/model_plot.png', show_layer_names=True)"],"metadata":{"id":"NaJ4kxq0GC4T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **CREAZIONE MODELLO: Resnet50**"],"metadata":{"id":"7AFRfO8g_6pb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhbMBYbQBvb9"},"outputs":[],"source":["# creo il modello\n","resnet = ResNet50(include_top=False, input_shape=(224,224,3))\n","x = Flatten()(resnet.output)\n","prediction = Dense(1, activation='sigmoid')(x)\n","model = Model(inputs=resnet.input, outputs=prediction)\n","\n","\n","# compilo il modello\n","model.compile(\n","  loss='binary_crossentropy',\n","  optimizer=Adam(learning_rate=1e-4),\n","  metrics=['accuracy', AUC(name='auc')]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMjXsgKzrFqn"},"outputs":[],"source":["from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file=results_dir+'Resnet50/model_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"FTmXy_bDLa-3"},"source":["# **MODEL FITTING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NwkFYA9JyO1"},"outputs":[],"source":["# ----------------CALLBACKS------------------------------------\n","early_stop = EarlyStopping(\n","    monitor='loss',\n","    patience=5,\n","    verbose=1,\n","    restore_best_weights=True\n",")"]},{"cell_type":"code","source":["EPOCHS = 30\n","\n","# Dizionario per plottare l'andamento di loss, accuracy e auc durante il training del modello\n","hist = {'loss':[], 'accuracy':[], 'auc':[], 'val_loss':[], 'val_accuracy':[], 'val_auc':[]}\n","\n","yt = []\n","yp = []\n","fold = 1\n","\n","# Timer per misurare il tempo totale di training\n","t0 = time.time()\n","\n","for train,val in StratifiedKFold(8, shuffle=True).split(train_data,train_target):\n","  print(f'\\n\\n\\n\\n------------- Fold #{fold} as validation set ---------------')\n","\n","  # divido il dataset in training e validation folds\n","  x_train, x_val, y_train, y_val = train_data[train], train_data[val], \\\n","                                  train_target[train], train_target[val],\n","\n","  history = model.fit(\n","    x_train,\n","    y_train,\n","    validation_data=(x_val, y_val),\n","    epochs=EPOCHS,\n","    class_weight=class_weights,\n","    verbose=1,\n","    shuffle=True,\n","    callbacks=[early_stop]\n","  )\n","\n","  hist['loss'].append(np.mean(history.history['loss']))\n","  hist['accuracy'].append(np.mean(history.history['accuracy']))\n","  hist['auc'].append(np.mean(history.history['auc']))\n","  hist['val_loss'].append(np.mean(history.history['val_loss']))\n","  hist['val_accuracy'].append(np.mean(history.history['val_accuracy']))\n","  hist['val_auc'].append(np.mean(history.history['val_auc']))\n","\n","  y_pred = model.predict(x_val)\n","\n","  yt.append(y_val)\n","  yp.append(y_pred)\n","  fold += 1\n","\n","t1 = time.time()\n","training_time = int(t1-t0)\n","print('Tempo di training: ', training_time)\n","\n","yt = np.concatenate(yt)\n","yp = np.concatenate(yp)"],"metadata":{"id":"Y0L7NPlDUeSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res_fold = results_dir + f'Resnet50/4mal_1ben_AUG_{EPOCHS}epochs_tt{training_time}/'\n","os.mkdir(res_fold)"],"metadata":{"id":"jnULorja-LIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(hist)\n","print(df)\n","dfi.export(df,res_fold+'train_df',table_conversion = 'matplotlib')"],"metadata":{"id":"d1c96sUAdkGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot della accuracy\n","plt.title('Training and Validation Accuracy')\n","plt.plot(hist['accuracy'], label='train accuracy')\n","plt.plot(hist['val_accuracy'], label='val accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('accuracy')\n","plt.xlabel('Fold iterations')\n","plt.savefig(res_fold+'train_acc')\n","plt.show()\n","\n","print('\\n\\n\\n')\n","\n","# plot della loss\n","plt.title('Training and Validation Loss')\n","plt.plot(hist['loss'], label='train loss')\n","plt.plot(hist['val_loss'], label='val loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Loss')\n","plt.xlabel('Fold iterations')\n","plt.savefig(res_fold+'train_loss',format='png')\n","plt.show()\n","\n","print('\\n\\n\\n')\n","\n","# plot della auc\n","plt.title('Training and Validation AUC')\n","plt.plot(hist['auc'], label='train AUC')\n","plt.plot(hist['val_auc'], label='val AUC')\n","plt.legend(loc='lower right')\n","plt.ylabel('AUC')\n","plt.xlabel('Fold iterations')\n","plt.savefig(res_fold+'train_auc',format='png')\n","plt.show()"],"metadata":{"id":"-adbOl_V1MI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CONFUSION MATRIX\n","def plot_confusion_matrix(labels, predictions, title='', filename='confusion_matrix'):\n","  cm = confusion_matrix(labels, np.round(predictions))\n","  ConfusionMatrixDisplay.from_predictions(labels, np.round(predictions), cmap=plt.cm.OrRd, display_labels=['Benigno','Maligno'])\n","  plt.title(title + ' Confusion Matrix')\n","  plt.savefig(res_fold+filename,format='png')\n","  plt.show()\n","  return cm\n","\n","\n","plot_confusion_matrix(yt, yp,'Training','train_cm')"],"metadata":{"id":"utCIGWbp00g3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKHMRu6vROac"},"source":["# **ANALISI DEI RISULTATI**"]},{"cell_type":"code","source":["y_pred = model.predict(test_data)\n","\n","# ROC CURVE\n","fpr,tpr,_ = roc_curve(test_target, y_pred)\n","plt.plot(fpr,tpr, marker='.', label='ROC Curve')\n","plt.plot([0,1],[0,1], 'y--', label='Random Classifier')\n","plt.grid(True)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend(loc='lower right')\n","plt.title('ROC Curve')\n","plt.savefig(res_fold+'test_ROC',format='png')\n","plt.show()\n","\n","print('\\n\\n\\n')\n","\n","\n","#PRECISON RECALL CURVE\n","precision, recall,_ = precision_recall_curve(test_target, y_pred)\n","plt.plot(recall, precision, marker='.')\n","plt.grid(True)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.savefig(res_fold+'test_prec_recall',format='png')\n","plt.show()\n","\n","print('\\n\\n\\n')\n","\n","\n","#CONFUSION MATRIX\n","cm = plot_confusion_matrix(test_target, y_pred, 'Test', 'test_cm')"],"metadata":{"id":"eifs5DTz0kOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TN = cm[0,0]\n","FP = cm[0,1]\n","FN = cm[1,0]\n","TP = cm[1,1]\n","\n","\n","bal_acc = balanced_accuracy_score(test_target, np.round(y_pred))\n","print('Balanced accuracy = ',bal_acc)\n","\n","############## Integral Metrics #########################\n","auc_value = auc(fpr, tpr)\n","print('\\nArea Under the Curve (AUC) = ', auc_value)\n","\n","\n","selected_points = np.where(tpr > 0.8) # seleziona i punti della curva ROC in cui la sensitivity è maggiore dell'80%\n","auc_sens80 = auc(fpr[selected_points], tpr[selected_points])\n","print('AUC, Sens. > 80% = ', auc_sens80)\n","\n","\n","avg_prec_score = average_precision_score(test_target, y_pred)\n","print('Average Precision Score = ',avg_prec_score)\n","\n","############### Threshold Metrics #########################\n","accuracy = accuracy_score(test_target, np.round(y_pred))\n","print('\\nAccuracy = ', accuracy)\n","\n","sensitivity = TP / (TP+FN)\n","print('Sensitivity = ', sensitivity)    # TPR / recall\n","\n","specificity = TN / (TN+FP)\n","print('Specificity = ', specificity)\n","\n","f1 = f1_score(test_target, np.round(y_pred))\n","print('Dice Coefficient = ', f1)\n","\n","PPV = TP/(TP+FP)\n","print('PPV = ', PPV)    # Positive predictive value or precision\n","\n","NPV = TN/(TN+FN)\n","print('NPV = ', NPV)    # Negative predictive value\n","\n","\n","\n","# salvo i risultati come un immagine\n","dict = {'Balanced accuracy': bal_acc,\n","        'AUC': auc_value,\n","        'AUC, Sens>80%': auc_sens80,\n","        'Average Precision': avg_prec_score,\n","        'Accuracy': accuracy,\n","        'Sensitivity': sensitivity,\n","        'Specificity': specificity,\n","        'Dice Coefficient': f1,\n","        'PPV': PPV,\n","        'NPV': NPV\n","        }\n","\n","df = pd.DataFrame.from_dict(dict, orient='index',columns=['value'])\n","dfi.export(df,res_fold+'results',table_conversion = 'matplotlib')"],"metadata":{"id":"DQw1vPEMKG7Q"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"private_outputs":true,"machine_shape":"hm","gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}